% Chapter Template

\chapter{Introduction} % Main chapter title

\label{ch:introduction} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

Surface normal is an important property of a surface with many applications, like surface reconstruction, shadings generation and other visual effects. The quality of the surface normal is directly impact the performances of the techniques mentioned above. However, especially in the task of real-world object digitalization, the surface is usually hardly to be mathematically described in equations due to the elaborate details on the objects. Instead, it is common to use a group of points to describe the object surface, which is a memory economical solution and also can be easier measured by 3D scanners.  The working principle of scanners are various due to the application scenarios, which consequently produce different point cloud structure. For the scanners without positions recording, the point cloud acquired after scanning is unstructured. In this case, every 3D point can be captured by different capture position, and neighbors are not defined by capture time, which increase the difficulty and computation for the neighbor based normal inference approaches.  Furthermore, since the lack of inherent structure, the normal can hardly be inferred by the parallel approaches. 
To acquire the structured point cloud, a depth camera can be used for data collection. It captures the RGB-D images for the object, which includes the standard RGB image with depth information of each pixel. After camera calibration, the corresponding point cloud can be calculated based on the depth map and camera matrix. The advantages are, a structured point cloud is mapped directly based on the same capture position from a 2D depth map, the neighbor information of each point is identical to the corresponding pixel in the 2D depth map, which reduced a huge computational works. Besides, base on the RGB image, which has the same indices with depth map, the normal inference task can also utilize the relationship between surface normal and light reflection to further rectify the inferred normal map with knowing light source. Furthermore, the structured point cloud has the same shape with the standard RGB images, which can be used for deep learning based approaches. 
Unfortunately, the depth maps captured by the sensors are only semi-dense, which is mainly caused by optical noises and the reflections in dark and shinny areas. Consequently, it raises the challenges to the accuracy of normal inference tasks. 
Recently, deep learning-based methods are used in computer vision tasks such as image segmentation, image inpainting, and depth density enhancement. The output of the model is usually has the same shape as the input but with different content, which is by comparison similar to the normal inference task. 
The deep learning based method has many advantages, first, it deals well with semi-dense input, which is a common issue for the depth map. Second, the network of deep learning model handles the structured point cloud as a single input and infer the corresponding normal all together, which is very time economical comparing to the approaches like neighbor based methods. Furthermore, if the texture image acquiring from the camera is already illuminated by strong directional light of a video projector, whose position is known, then there should exist theoretical relations among light direction, normal direction, and grayscale image. Then the normal can be inferred better using the given image information and depth map.  Based on the grayscale image and corresponding semi-dense depth maps, a multi-staged model can be designed to infer the normal map, which can give more density and robust results comparing to the standard algorithm.
In this work, we apply the deep learning based approach for surface normal inference, which is based on the calibrated illuminated RGB-D image. A network named gated convolution neural network (GCNN) is proposed for normal inference, which is robust to the semi-dense input. 
With the help of synthetic data in Unity, a dataset is also created for CNN model training. It can provide accurate ground truth for training work, which real data is usually not provided. The trained model is performed well both in synthetic dataset as well as the real dataset captured from RGB-D cameras and achieve a remarkably better prediction accuracy at a low computational cost compared to the standard approaches for semi-dense point clouds.  
The structure of the thesis is as follows, Chapter 1 is the introduction of the whole work. Chapter 2 briefly discusses the related work about normal inference. Chapter 3 is the main approaches of this work. Chapter 4 introduces the created dataset for the training work. Chapter 5 is the description of the experiments and the evaluation of the models. Chapter 6 is the conclusion of the whole thesis.


