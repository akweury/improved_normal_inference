% Chapter Template

\chapter{Conclusion} % Main chapter title

\label{ch:06} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------
In our approaches, two networks have been proposed. The first network called GCNN is used for geometry information based approach, which is our baseline method. It is especially good for the semi-dense input data which is benefited with gated convolution layer design.  Using this network we achieved a more accurate normal inference than using standard layers network based on the same architecture. We also show the effectiveness of skip connection in the U-Net. Besides, the fully convolution design enables it to use various resolution images as input. 

The second network is called Trip-Net, which is based on the GCNN model. It is designed for illumination calibrated RGB-D image based approaches. This model utilises GCNN architecture three times in order to handle vertex map, light map and image map separately. We have also found that 3 to 4 times fusion times among 3 pipes in our models is the key point for the performance enhancement.
Based on our experiments, it shows that the models trained on illuminated calibrated RGB-D images achieve a  better performance than geometry information only based approach and gives more sharpness in the normal map.

We also explored the performance of our models on a real dataset and compare with traditional method like SVD based approach. Especially, our gated convolution layer based model can fill the missing pixels in the real input data and keep an accurate normal estimation, whereas the optimization based approach like SVD is failed to do so. 


We collected 55 high density point clouds from internet and used them for our dataset generation. Specifically, we use a RGB-D camera, an ambient light and one directional light  collect thousands of scenes. The scene generation is simulated by game engine Unity. Furthermore, we also simulated noise in our synthetic dataset by a uniform distributed drop off to fit our model and adopt it with semi-dense input data.
However, for the large missing areas, we did not generate a similar noise in the training set, which leads to our model cannot fill the holes in the real data. Exploring a sensor-noise mimic algorithm to generate more reality closed data for training work can be a direction of further work.