% Chapter Template

\chapter{Conclusion} % Main chapter title

\label{ch:06} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

In our approaches, two networks have been proposed. The first network called GCNN is used for the geometry information based approach, which is our baseline method. It is particularly well suited for semi-dense input data, which is supported by the gated convolution layer design.  With this network, we have achieved more accurate normal inference than with a standard layered network based on the same architecture. We also show the effectiveness of skip connectivity in the U-Net. In addition, the full convolution design allows us to use images with different resolutions as input. 

The second network is called Trip-Net and is based on the GCNN model. It is designed for illumination-calibrated RGB-D image-based approaches. This model uses the GCNN architecture three times to process vertex map, light map and image separately. We also found that 4 fusions of the 3 pipes in our models is the key point to improve the performance.
Based on our proposed dataset with resolution $ 128\times 128 $, the experiments show that the models trained on illuminated calibrated RGB-D images improve the accuracy by $ 11.1\% $ of the average degree error compared to the geometry information based approach, which provides more sharpness in the normal map.


We also studied the performance of our models on a real dataset and compared it with traditional methods such as the SVD-based approach. In particular, our gated convolution layer-based model can fill in the missing pixels in the real input data and maintain an accurate normal estimate, while the optimization-based approach such as SVD cannot. 


We collected 55 high-resolution 3D models from the Internet and used them to create our data. Specifically, we use an RGB-D camera, an ambient light, and a directional light to collect thousands of scenes. The scene generation is simulated by the Unity game engine. In addition, we also simulated noise in our synthetic dataset using an uniformly distributed drop to fit our model and adopt it with semi-dense input data.
However, for the large missing regions, we did not generate similar noise in the training set, which results in our model not being able to fill the holes in the real data. Exploring an algorithm to mimic sensor noise to generate more realistic data for training can be a direction for further work.
