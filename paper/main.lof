\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax 
\babel@toc {english}{}\relax 
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1.1}{\ignorespaces Left: A part of the point cloud of the dragon model. Right: The zoom in of the left point cloud.\relax }}{1}{figure.caption.13}%
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.1}{\ignorespaces Intrinsic image analysis of the bus object. From left to right, original image, reflectance image, shading image, light image, normal image.\relax }}{7}{figure.caption.28}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.2}{\ignorespaces Left: the light reflection on a lambertian surface. (image source \cite {lambertian-reflectance}). Right: The surface normal, source light direction and the view point direction, where $ \theta $ denotes the angle between light direction and the normal.\relax }}{8}{figure.caption.29}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.3}{\ignorespaces The structure of UNet. \cite {unet}\relax }}{9}{figure.caption.31}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.4}{\ignorespaces Gated Convolution Layer, where $ \oplus $ denotes element-wise multiplication.\relax }}{11}{figure.caption.35}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.6}{\ignorespaces Three kinds of input data. From left to right, vertex map, light map, gray-scale image.\relax }}{13}{figure.caption.44}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.8}{\ignorespaces The shape of Berhu Loss (show in red line). \relax }}{16}{figure.caption.55}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.5}{\ignorespaces The architecture of Gated convolution neural network (GCNN) based on Gated convolution and UNet Architecture.\relax }}{17}{figure.caption.37}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.7}{\ignorespaces The architecture of Trig-Net\relax }}{18}{figure.caption.49}%
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.1}{\ignorespaces Point clouds scanned by high resolution scanners\relax }}{20}{figure.caption.58}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.2}{\ignorespaces The layout of synthetic scenes generation in Unity.\relax }}{21}{figure.caption.60}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.3}{\ignorespaces Different exposure to the objects.\relax }}{22}{figure.caption.62}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.4}{\ignorespaces Convert depth to point in camera coordinate system\relax }}{23}{figure.caption.65}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.5}{\ignorespaces Left: Extreme value in 3 axis; Right: Vertex range in 3 axis\relax }}{24}{figure.caption.67}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.6}{\ignorespaces Noise-intensity on $ \mu -0, \mu -10,\mu -20, \mu -30, \mu -40, \mu -50,$. Object Name: elephant-zun-lid.\relax }}{24}{figure.caption.70}%
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.1}{\ignorespaces Some of the test scenes during the training. From top to bottom, baoshanlu, Washington, Garfield, Dragon, Bus\relax }}{27}{figure.caption.75}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.2}{\ignorespaces Normal map of a dragon object predicted by neighbor based method. k=2, angle error=5 \textbf {Left}: ground-truth normal map \textbf {Middle}: predicted normal map, \textbf {Right}: Error map\relax }}{31}{figure.caption.92}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.3}{\ignorespaces Error map of neighbor based method with different $ k $ values. From left to right, $ k=1,2,3,4 $ separately.\relax }}{32}{figure.caption.93}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.4}{\ignorespaces Evaluation of neighbor based method on a noised dragon model\relax }}{32}{figure.caption.94}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.5}{\ignorespaces Normal inference based on GCNN. Test image has resolution $ 128\times 128 $\relax }}{33}{figure.caption.96}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.6}{\ignorespaces Zoom in of some regions of Dragon object. The first row is surface normal, the second row is the corresponding errors. Zoom-in normal map corresponding $ 32\times 32 $ points in the original matrix.\relax }}{33}{figure.caption.97}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.7}{\ignorespaces Comparison of GCNN model with no skip connection version(NOC) and standard convolution layer only version (CNN). The second row is the corresponding mean average degree error.\relax }}{34}{figure.caption.98}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.8}{\ignorespaces Evaluation on objects Baoshanlu, Washington statue, Bus(from top to bottom).\relax }}{35}{figure.caption.99}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.9}{\ignorespaces Normal inference based on Trip-Net. Test image has resolution $ 128\times 128 $\relax }}{36}{figure.caption.101}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.10}{\ignorespaces Zoom in of some regions of Dragon object. The first row is surface normal, the second row is the corresponding errors. Zoom-in normal map corresponding $ 32\times 32 $ points in the original matrix.\relax }}{36}{figure.caption.102}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.11}{\ignorespaces Comparison between different fusion times for Trip-Net. The first row is surface normal, the second row is the corresponding errors. The number in ``F$ x $" represents the fusion times. \relax }}{37}{figure.caption.103}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.12}{\ignorespaces Evaluation on objects Baoshanlu, Washington statue, Bus(from top to bottom).\relax }}{38}{figure.caption.104}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.13}{\ignorespaces Normal inference based on Trip-Net. Test image has resolution $ 128\times 128 $\relax }}{39}{figure.caption.106}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.14}{\ignorespaces Comparison between different SVD, GCNN and Trip-Net model on $ 512\times 512 $ dataset. The first row is surface normal, the second row is the corresponding errors. \relax }}{40}{figure.caption.108}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.15}{\ignorespaces Normal inference based on Trip-Net. Test image has resolution $ 512\times 512 $\relax }}{41}{figure.caption.110}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.16}{\ignorespaces Comparison between different SVD, GCNN and Trip-Net model on $ 512\times 512 $ real dataset. The first row is surface normal, the second row is the corresponding errors. \relax }}{42}{figure.caption.112}%
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {A.1}{\ignorespaces Point clouds in training dataset A \relax }}{45}{figure.caption.116}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {A.2}{\ignorespaces Point clouds in training dataset B \relax }}{46}{figure.caption.117}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {A.3}{\ignorespaces Point clouds in training dataset C\relax }}{47}{figure.caption.118}%
