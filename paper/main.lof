\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax 
\babel@toc {english}{}\relax 
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1.1}{\ignorespaces Left: A part of the point cloud of the dragon model. Right: The zoom in of the left point cloud.\relax }}{1}{figure.caption.13}%
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.1}{\ignorespaces Normal map of a dragon object predicted by neighbor based method. k=2, angle error=5 \textbf {Left}: ground-truth normal map \textbf {Middle}: predicted normal map, \textbf {Right}: Error map\relax }}{6}{figure.caption.28}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.2}{\ignorespaces Error map of neighbor based method with different $ k $ values. From left to right, $ k=1,2,3,4 $ separately.\relax }}{7}{figure.caption.29}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.3}{\ignorespaces Evaluation of neighbor based method on a noised dragon model\relax }}{7}{figure.caption.30}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.4}{\ignorespaces Intrinsic image analysis of the bus object. From left to right, original image, reflectance image, shading image, light image, normal image\relax }}{8}{figure.caption.33}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.5}{\ignorespaces The surface normal, source light direction and the view point direction, where $ \theta $ denotes the angle between light direction and the normal.\relax }}{8}{figure.caption.34}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.6}{\ignorespaces The structure of UNet. \cite {unet}\relax }}{9}{figure.caption.36}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.7}{\ignorespaces Gated Convolution Layer, where $ \oplus $ denotes element-wise multiplication.\relax }}{11}{figure.caption.40}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.9}{\ignorespaces The light map calculated from vertex map and the light source\relax }}{13}{figure.caption.53}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.8}{\ignorespaces The architecture of Gated convolution neural network (GCNN) based on Gated convolution and UNet Architecture.\relax }}{16}{figure.caption.42}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.10}{\ignorespaces The architecture of Trig-Net\relax }}{17}{figure.caption.59}%
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.1}{\ignorespaces Point clouds scanned by high resolution scanners\relax }}{20}{figure.caption.63}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.2}{\ignorespaces The layout of synthetic scenes generation in Unity.\relax }}{20}{figure.caption.65}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.3}{\ignorespaces Convert depth to point in camera coordinate system\relax }}{21}{figure.caption.68}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.4}{\ignorespaces Left: Extreme value in 3 axis; Right: Vertex range in 3 axis\relax }}{22}{figure.caption.70}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.5}{\ignorespaces Noise-intensity on $ \mu -0, \mu -10,\mu -20, \mu -30, \mu -40, \mu -50,$. Object Name: elephant-zun-lid.\relax }}{23}{figure.caption.73}%
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.1}{\ignorespaces Surface Normal Inference based GCNN model on "Dragon´´ object. The first row shows the estimated surface normal. The second row is the angle error map.\relax }}{25}{figure.caption.78}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.2}{\ignorespaces Zoom in of the center region of Dragon object. The first row is surface normal, the second row is the corresponding errors. NOC model has no skip connnection, CNN model replace gated convolution layer to standard convolution layer.\relax }}{26}{figure.caption.79}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.3}{\ignorespaces TripNet qualitative evaluation. Surface Normal Inference on ``Dragon" object. The first row shows the estimated surface normal. The second row is the angle error map.\relax }}{27}{figure.caption.82}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.4}{\ignorespaces Zoom in of the center region of Dragon object. The first row is surface normal, the second row is the corresponding errors.\relax }}{27}{figure.caption.83}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.5}{\ignorespaces Evaluation on objects Baoshanlu, Washington statue, Bus(from top to bottom).\relax }}{29}{figure.caption.86}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.6}{\ignorespaces Zoom in of the center region of the objects in Figure \ref {fig:final-eval}\relax }}{30}{figure.caption.87}%
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {A.1}{\ignorespaces Point clouds in training dataset A \relax }}{34}{figure.caption.91}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {A.2}{\ignorespaces Point clouds in training dataset B\relax }}{35}{figure.caption.92}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {A.3}{\ignorespaces Point clouds in training dataset C\relax }}{36}{figure.caption.93}%
