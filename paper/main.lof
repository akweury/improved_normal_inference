\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax 
\babel@toc {english}{}\relax 
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1.1}{\ignorespaces Left: A part of the point cloud of the dragon model. Right: The zoom in of the left point cloud.\relax }}{1}{figure.caption.13}%
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.1}{\ignorespaces Normal map of a dragon object predicted by neighbor based method. k=2, angle error=5 \textbf {Left}: ground-truth normal map \textbf {Middle}: predicted normal map, \textbf {Right}: Error map\relax }}{6}{figure.caption.28}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.2}{\ignorespaces Error map of neighbor based method with different $ k $ values. From left to right, $ k=1,2,3,4 $ separately.\relax }}{7}{figure.caption.29}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.3}{\ignorespaces Evaluation of neighbor based method on a noised dragon model\relax }}{7}{figure.caption.30}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.4}{\ignorespaces Intrinsic image analysis of the bus object. From left to right, original image, reflectance image, shading image, light image, normal image\relax }}{8}{figure.caption.33}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.5}{\ignorespaces The surface normal, source light direction and the view point direction, where $ \theta $ denotes the angle between light direction and the normal.\relax }}{8}{figure.caption.34}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.6}{\ignorespaces The structure of UNet. \cite {unet}\relax }}{9}{figure.caption.36}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.7}{\ignorespaces Gated Convolution Layer, where $ \oplus $ denotes element-wise multiplication.\relax }}{11}{figure.caption.40}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.9}{\ignorespaces The light map calculated from vertex map and the light source\relax }}{13}{figure.caption.53}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.8}{\ignorespaces The architecture of Gated convolution neural network (GCNN) based on Gated convolution and UNet Architecture.\relax }}{16}{figure.caption.42}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.10}{\ignorespaces The architecture of Trig-Net\relax }}{17}{figure.caption.59}%
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.1}{\ignorespaces Point clouds scanned by high resolution scanners\relax }}{20}{figure.caption.63}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.2}{\ignorespaces The layout of synthetic scenes generation in Unity.\relax }}{20}{figure.caption.65}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.3}{\ignorespaces Convert depth to point in camera coordinate system\relax }}{21}{figure.caption.68}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.4}{\ignorespaces Left: Extreme value in 3 axis; Right: Vertex range in 3 axis\relax }}{22}{figure.caption.70}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.5}{\ignorespaces Noise-intensity on $ \mu -0, \mu -10,\mu -20, \mu -30, \mu -40, \mu -50,$. Object Name: elephant-zun-lid.\relax }}{23}{figure.caption.73}%
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.1}{\ignorespaces Some of the test scenes during the training. From top to bottom, baoshanlu, Washington, Garfield, Dragon, Bus\relax }}{26}{figure.caption.78}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.2}{\ignorespaces The training history of GCNN model. The line chart records the training BerHu loss of the model GCNN, NOC and CNN. The left shows the training loss history whereas the right one shows the evaluation loss history.\relax }}{27}{figure.caption.80}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.3}{\ignorespaces The training history of Trip-Net model. The line chart records the training BerHu loss of the model Trip-Net, Trip-Net-F1, Trip-Net-F2, Trip-Net-F3, GCNN.\relax }}{28}{figure.caption.82}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.4}{\ignorespaces Normal inference based on GCNN. Test image has resolution $ 128\times 128 $\relax }}{31}{figure.caption.98}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.5}{\ignorespaces Zoom in of some regions of Dragon object. The first row is surface normal, the second row is the corresponding errors. Zoom-in normal map corresponding $ 32\times 32 $ points in the original matrix.\relax }}{31}{figure.caption.99}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.6}{\ignorespaces Comparison of GCNN model with no skip connection version(NOC) and standard convolution layer only version (CNN). The second row is the corresponding mean average degree error.\relax }}{31}{figure.caption.100}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.7}{\ignorespaces Normal inference based on Trip-Net. Test image has resolution $ 128\times 128 $\relax }}{32}{figure.caption.102}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.8}{\ignorespaces Zoom in of some regions of Dragon object. The first row is surface normal, the second row is the corresponding errors. Zoom-in normal map corresponding $ 32\times 32 $ points in the original matrix.\relax }}{32}{figure.caption.103}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.9}{\ignorespaces Evaluation on objects Baoshanlu, Washington statue, Bus(from top to bottom).\relax }}{33}{figure.caption.105}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.10}{\ignorespaces Zoom in of the center region of the objects in Figure \ref {fig:final-eval}\relax }}{34}{figure.caption.106}%
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {A.1}{\ignorespaces Point clouds in training dataset A \relax }}{38}{figure.caption.110}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {A.2}{\ignorespaces Point clouds in training dataset B\relax }}{39}{figure.caption.111}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {A.3}{\ignorespaces Point clouds in training dataset C\relax }}{40}{figure.caption.112}%
